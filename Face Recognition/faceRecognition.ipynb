{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceRecognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f3e33aded7c4d34a5cc87b0108ce0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b665227b6a4412c85192673415c1d03",
              "IPY_MODEL_a9e43b65e99f41a7b068b118ae320c44"
            ],
            "layout": "IPY_MODEL_85a0b6f15da34d5688395653b40f46c9"
          }
        },
        "5b665227b6a4412c85192673415c1d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3edd71df4278481cae42cbb5f396e539",
            "placeholder": "​",
            "style": "IPY_MODEL_069fe56b81e24332bb65b659f1453836",
            "value": "0.015 MB of 0.015 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "a9e43b65e99f41a7b068b118ae320c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b650399fc2a4111ba464dbd0f6ea1c6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b641d1ddfff4d9e9ec39fa2c497652b",
            "value": 1
          }
        },
        "85a0b6f15da34d5688395653b40f46c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3edd71df4278481cae42cbb5f396e539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "069fe56b81e24332bb65b659f1453836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b650399fc2a4111ba464dbd0f6ea1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b641d1ddfff4d9e9ec39fa2c497652b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenyaminZojaji/Deep_Learning/blob/main/Face%20Recognition/faceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ3uoGYoCz7t"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "Jmhr05wwFbE5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wandb configuration"
      ],
      "metadata": {
        "id": "GUPFyVpUK0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "width = height = 224"
      ],
      "metadata": {
        "id": "oNZLu4PIGg1g"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "              \"learning_rate\": 0.001,\n",
        "              \"epochs\": 25,\n",
        "              \"batch_size\": 32,\n",
        "              \"log_step\": 200,\n",
        "              \"val_log_step\": 50\n",
        "           }\n",
        "run = wandb.init(project='faceRecognition', config=config)\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "pQOXOlAIQtHn",
        "outputId": "21ac934c-6e57-40f4-efc4-df324e42075a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenyaminzojaji\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220402_130910-186718cy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/benyaminzojaji/faceRecognition/runs/186718cy\" target=\"_blank\">faithful-bush-11</a></strong> to <a href=\"https://wandb.ai/benyaminzojaji/faceRecognition\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klgWa7agMoez",
        "outputId": "6d980032-18c3-42cc-c052-beff44e1e95c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "HNWrLP9xK6U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/7-7 dataset'\n",
        "\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1.0/255.0,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.8,1.2),\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_data = idg.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(width, height),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = idg.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(width, height),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVIS2ck-GsLF",
        "outputId": "6ef04c57-64c2-40cd-8cf9-778f7c565f2e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1090 images belonging to 14 classes.\n",
            "Found 268 images belonging to 14 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self, number_of_classes):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense_1 = Dense(128, activation='relu')\n",
        "    self.dense_2 = Dense(256, activation='relu')\n",
        "    self.dense_3 = Dense(number_of_classes, activation='softmax')\n",
        "    self.conv2d_1 = Conv2D(32, (3, 3), activation='relu', input_shape = (width, height, 3))\n",
        "    self.conv2d_2 = Conv2D(64, (5, 5), activation='relu')\n",
        "    self.conv2d_3 = Conv2D(128, (5, 5), activation='relu')\n",
        "    self.conv2d_4 = Conv2D(256, (3, 3), activation='relu')\n",
        "    self.maxpool = MaxPool2D()\n",
        "    self.flatten = Flatten()\n",
        "    self.dropout1 = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv2d_1(x)\n",
        "    #x = self.conv2d_2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2d_3(x)\n",
        "    #x = self.conv2d_4(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout1(x)\n",
        "    #x = self.dense_1(x)\n",
        "    #x = self.dense_2(x)\n",
        "    out = self.dense_3(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "qHseKVwEH36f"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer =  tf.keras.optimizers.Adam(config.learning_rate)"
      ],
      "metadata": {
        "id": "lLMRYJSxP04E"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.MeanAbsoluteError()\n",
        "test_loss = tf.keras.metrics.MeanAbsoluteError()\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "bsho2CoqQO1m"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(14)"
      ],
      "metadata": {
        "id": "JUUfYp1IR1oJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as gTape:\n",
        "    predictions = model(images, training = True)\n",
        "    loss= loss_function(labels, predictions)\n",
        "    train_loss(labels, predictions)\n",
        "    train_accuracy(labels, predictions)\n",
        "  \n",
        "  # calculate gradients\n",
        "  gradients = gTape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # update weights\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "xgWXXu-LQeKx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(images, labels):\n",
        "  predictions = model(images, training = False)\n",
        "  loss= loss_function(labels, predictions)\n",
        "  test_accuracy(labels, predictions)\n",
        "  test_loss(labels, predictions)"
      ],
      "metadata": {
        "id": "jzrdJcvyTDLg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    epochs = 15\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss.reset_state()\n",
        "        test_loss.reset_state()\n",
        "        train_accuracy.reset_state()\n",
        "        test_accuracy.reset_state()\n",
        "        print('epoch:', epoch + 1)\n",
        "\n",
        "        for index, (images, labels) in enumerate(tqdm(train_data)):\n",
        "            train_step(images, labels)\n",
        "            if len(train_data) <= index:\n",
        "              break\n",
        "\n",
        "        for index, (images, labels) in enumerate(tqdm(val_data)):\n",
        "            test_step(images, labels)\n",
        "            if len(val_data) <= index:\n",
        "              break\n",
        "        \n",
        "        print('accuracy:', train_accuracy.result())\n",
        "        print('val accuracy:', test_accuracy.result())\n",
        "        print('loss:', train_loss.result())\n",
        "        print('val loss:', test_loss.result())\n",
        "\n",
        "        wandb.log({'epochs': epoch,\n",
        "            'train_loss': np.mean(train_loss.result()),\n",
        "            'train_accuracy': float(train_accuracy.result()), \n",
        "            'test_loss': np.mean(test_loss.result()),\n",
        "            'test_accuracy': float(test_accuracy.result())\n",
        "            })"
      ],
      "metadata": {
        "id": "JpaXAv-sThmv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "dtLuExwpVNiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5f3e33aded7c4d34a5cc87b0108ce0af",
            "5b665227b6a4412c85192673415c1d03",
            "a9e43b65e99f41a7b068b118ae320c44",
            "85a0b6f15da34d5688395653b40f46c9",
            "3edd71df4278481cae42cbb5f396e539",
            "069fe56b81e24332bb65b659f1453836",
            "4b650399fc2a4111ba464dbd0f6ea1c6",
            "1b641d1ddfff4d9e9ec39fa2c497652b"
          ]
        },
        "outputId": "ce764a0e-b586-45f6-d709-237c487f2d8a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.24s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.27005348, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.41666666, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.11897589, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.09894117, shape=(), dtype=float32)\n",
            "epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.21s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.62923354, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.5833333, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.076102555, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.0710448, shape=(), dtype=float32)\n",
            "epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.22s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.7798574, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.66333336, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.049553618, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.059089694, shape=(), dtype=float32)\n",
            "epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.82887703, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.7966667, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.03804805, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.0486635, shape=(), dtype=float32)\n",
            "epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.22s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.8903743, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.7366667, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.026350215, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.04390659, shape=(), dtype=float32)\n",
            "epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.8939394, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.78, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.023605336, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.040988937, shape=(), dtype=float32)\n",
            "epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.9188948, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.75333333, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.018957319, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.039756544, shape=(), dtype=float32)\n",
            "epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.22s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.9197861, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.76666665, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.019092783, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.03974855, shape=(), dtype=float32)\n",
            "epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.93048126, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.8035714, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.015599829, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.032307748, shape=(), dtype=float32)\n",
            "epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.22s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.9322638, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.78, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.015375474, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.038848516, shape=(), dtype=float32)\n",
            "epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.24s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.9438503, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.7866667, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.013736341, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.035281505, shape=(), dtype=float32)\n",
            "epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.9456328, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.8233333, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.013495598, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.032765657, shape=(), dtype=float32)\n",
            "epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:43<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.956328, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.77666664, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.010422657, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.032956406, shape=(), dtype=float32)\n",
            "epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.23s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.94741535, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.78, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.010276248, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.03607814, shape=(), dtype=float32)\n",
            "epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:42<00:00,  1.21s/it]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tf.Tensor(0.96880573, shape=(), dtype=float32)\n",
            "val accuracy: tf.Tensor(0.84, shape=(), dtype=float32)\n",
            "loss: tf.Tensor(0.00738422, shape=(), dtype=float32)\n",
            "val loss: tf.Tensor(0.025144214, shape=(), dtype=float32)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f3e33aded7c4d34a5cc87b0108ce0af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>test_accuracy</td><td>▁▄▅▇▆▇▇▇▇▇▇█▇▇█</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>14</td></tr><tr><td>test_accuracy</td><td>0.84</td></tr><tr><td>test_loss</td><td>0.02514</td></tr><tr><td>train_accuracy</td><td>0.96881</td></tr><tr><td>train_loss</td><td>0.00738</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">faithful-bush-11</strong>: <a href=\"https://wandb.ai/benyaminzojaji/faceRecognition/runs/186718cy\" target=\"_blank\">https://wandb.ai/benyaminzojaji/faceRecognition/runs/186718cy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220402_130910-186718cy/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('faceRecognition', save_format='HDF5')"
      ],
      "metadata": {
        "id": "DdVutuxCYjd4"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}